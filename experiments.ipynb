{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic vector search for c code projects using microsoft/UniXcoder tokernizer\n",
    "\n",
    "1. Setup environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Proxy settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load Model to GPU (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda\n",
    "import torch.mps\n",
    "\n",
    "from unixcoder import UniXcoder\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Device used is: {device}\")\n",
    "\n",
    "model =  UniXcoder(\"microsoft/unixcoder-base-nine\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get C functions from project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import get_project_functions\n",
    "from pathlib import Path\n",
    "\n",
    "project_root_dir = Path(\"your path\")\n",
    "\n",
    "project_functions = list()\n",
    "\n",
    "if project_root_dir.exists():\n",
    "  project_functions = get_project_functions(project_root_dir)\n",
    "else:\n",
    "  print(\"No project found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Tokenize project functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ids = model.tokenize(project_functions, max_length=512, mode=\"<encoder-only>\", padding=True)\n",
    "\n",
    "print(len(tokens_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create embeddings for the functions. This is achieved creating tensors in the choosen device. This can consume a lot of memory device quickly, so we need to batch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_batch(my_list, batch_size):\n",
    "  for i in range(0, len(my_list), batch_size):\n",
    "    yield my_list[i:i + batch_size]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "function_idx = 0\n",
    "\n",
    "function_embeddings = []\n",
    "\n",
    "for token_batch in get_batch(tokens_ids, BATCH_SIZE):\n",
    "  \n",
    "  batch_token_tensor = torch.tensor(token_batch).to(device)\n",
    "  \n",
    "  batch_token_embeddings, batch_function_embeddings = model(batch_token_tensor)\n",
    "\n",
    "  # Normalize tensor with L2 norm\n",
    "  batch_function_embeddings = F.normalize(batch_function_embeddings, p=2, dim=1)\n",
    "\n",
    "  # Create embedding tensor list for FAISS indexing\n",
    "  batch_function_embeddings = batch_function_embeddings.tolist()\n",
    "\n",
    "  for function_embedding in batch_function_embeddings:\n",
    "    text_embedding = (project_functions[function_idx], function_embedding)\n",
    "    function_idx += 1\n",
    "    function_embeddings.append(text_embedding)\n",
    "\n",
    "print(function_embeddings[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Configure FAISS vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from typing import cast\n",
    "\n",
    "class MyEmbeddings(Embeddings):\n",
    "  def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "    tokens_ids = model.tokenize(texts, max_length=512, mode=\"<encoder-only>\", padding=True)\n",
    "\n",
    "    text_embeddings: list[list[float]] = []\n",
    "\n",
    "    for token_batch in get_batch(tokens_ids, BATCH_SIZE):\n",
    "      batch_token_tensor = torch.tensor(token_batch).to(device)\n",
    "      \n",
    "      batch_token_embeddings, batch_text_embeddings = model(batch_token_tensor)\n",
    "\n",
    "      # Normalize tensor with L2 norm\n",
    "      batch_text_embeddings = F.normalize(batch_text_embeddings, p=2, dim=1)\n",
    "      \n",
    "      # Create embedding tensor list for FAISS indexing\n",
    "      batch_text_embeddings = cast(list[list[float]], batch_text_embeddings.tolist())\n",
    "\n",
    "      text_embeddings.extend(batch_text_embeddings)\n",
    "\n",
    "    return text_embeddings\n",
    "\n",
    "  def embed_query(self, text: str) -> list[float]:\n",
    "    embedding = self.embed_documents([text])[0]\n",
    "    print(len(embedding))\n",
    "\n",
    "    return embedding\n",
    "\n",
    "\n",
    "vector_store = FAISS.from_embeddings(text_embeddings=function_embeddings, embedding=MyEmbeddings(), distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Query Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "query = \"your query\"\n",
    "\n",
    "query_results = vector_store.similarity_search_with_relevance_scores(\n",
    "  query=query,\n",
    "  k = top_k\n",
    ")\n",
    "\n",
    "print(len(query_results))\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(f\"Top {top_k} most similar functions in project:\")\n",
    "for query_entry in query_results[::-1]:\n",
    "  print(f\"(Score: {query_entry[1]:.4f})\\n\", query_entry[0])\n",
    "  print(\"==============\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
